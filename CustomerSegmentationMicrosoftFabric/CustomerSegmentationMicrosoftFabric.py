#!/usr/bin/env python
# coding: utf-8

# ## Notebook 1
# 
# New notebook

# In[1]:


# Welcome to your new notebook
# Type here in the cell editor to add code!
get_ipython().system('pip install Kaggle')


# In[2]:


import os
os.chdir('/lakehouse/default/Files')
os.environ['KAGGLE_USERNAME'] = 'bennyifeanyi'
os.environ['KAGGLE_KEY'] = '050019167fbe0027359cdb4b5eea50fe'
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()
api.dataset_download_file('vjchoudhary7/customer-segmentation-tutorial-in-python', 'Mall_Customers.csv')


# In[4]:


import pandas as pd
df = pd.read_csv("/lakehouse/default/" + "Files/Mall_Customers.csv")
df.head()


# In[8]:


# Code generated by Data Wrangler for pandas DataFrame

def clean_data(df):
    # Convert text to lowercase in column: 'Gender'
    df['Gender'] = df['Gender'].str.lower()
    # Rename column 'Spending Score (1-100)' to 'Spending Score'
    df = df.rename(columns={'Spending Score (1-100)': 'SpendingScore'})
    # Rename column 'Annual Income (k$)' to 'Annual Income'
    df = df.rename(columns={'Annual Income (k$)': 'AnnualIncome'})
    return df

df_clean = clean_data(df.copy())
df_clean.head()


# In[9]:


sparkdf = spark.createDataFrame(df_clean)
display(sparkdf)


# In[10]:


sparkdf.write.format("delta").mode("overwrite").saveAsTable("malldatadf")


# In[11]:


import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

X = df_clean[['AnnualIncome', 'SpendingScore']]
# Feature normalization
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)


# In[12]:


kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)
kmeans.fit(X_scaled)


# In[13]:


plt.figure(figsize=(10, 8))
for cluster_label in range(5):  # Loop through each cluster label
    cluster_points = X[kmeans.labels_ == cluster_label]
    centroid = cluster_points.mean(axis=0)  # Calculate the centroid as the mean position of the data points
    plt.scatter(cluster_points['AnnualIncome'], cluster_points['SpendingScore'],
                s=50, label=f'Cluster {cluster_label + 1}')  # Plot points for the current cluster
    plt.scatter(centroid[0], centroid[1], s=300, c='black', marker='*', label=f'Centroid {cluster_label + 1}')  # Plot the centroid
plt.title('Clusters of Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()


# In[18]:


# Create a new DataFrame to store the clustering results
cluster_df = pd.DataFrame(data=X, columns=['AnnualIncome', 'SpendingScore'])
cluster_df['Cluster'] = cluster_label
cluster_df.head()


# In[17]:


sparkclusterdf = spark.createDataFrame(cluster_df)
sparkclusterdf.write.format("delta").mode("overwrite").saveAsTable("clusterdatadf")

